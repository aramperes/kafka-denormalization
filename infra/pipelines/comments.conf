# This pipeline reads from the 'stories' index, and updates/upserts the parent object with the story metadata
input {
    kafka {
        bootstrap_servers => 'kafka:9092'
        topics => [ 'hn.comments' ]
        codec => 'json'
        group_id => 'logstash'
    }
}
filter {
    mutate {
        add_field => { "[@metadata][docId]" => "%{story}" }
    }
    ruby {
        code => 'event.to_hash.each { |k, v| event.set("[comments][" + k + "]", v) }'
    }
    prune {
        whitelist_names => [ '^comments$' ]
    }
}
output {
    stdout { codec => rubydebug { metadata => true } }
    elasticsearch {
        hosts => 'elasticsearch:9200'
        action => 'update'
        doc_as_upsert => true
        document_id => "%{[@metadata][docId]}"
        index => 'hn-stories-comments'

        # merge existing comments array
        script => '
            if (ctx._source.comments == null) {
                ctx._source.comments = new ArrayList();
            }

            if (ctx._source.comments instanceof Map) {
                List l = new ArrayList();
                l.add(ctx._source.comments);
                ctx._source.comments = l;
            }

            boolean f = false;
            for (int i = 0; i < ctx._source.comments.size(); i++) {
                if (ctx._source.comments[i].id == params.event.get("comments").id) {
                    ctx._source.comments[i] = params.event.get("comments");
                    f = true;
                    break;
                }
            }

            if (!f) {
                ctx._source.comments.add(params.event.get("comments"));
            }
            '
    }
}
