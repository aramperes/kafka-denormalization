# This pipeline reads from the 'stories' index, and updates/upserts the parent object with the story metadata
input {
    kafka {
        bootstrap_servers => 'kafka:9092'
        topics => [ 'hn.stories' ]
        codec => 'json'
        group_id => 'logstash'
    }
}
filter {
    mutate {
        add_field => { "[@metadata][docId]" => "%{id}" }
    }
    ruby {
        code => 'event.to_hash.each { |k, v| event.set("[story][" + k + "]", v) }'
    }
    prune {
        whitelist_names => [ '^story$' ]
    }
}
output {
    stdout { codec => rubydebug { metadata => true } }
    elasticsearch {
        hosts => 'elasticsearch:9200'
        action => 'update'
        doc_as_upsert => true
        document_id => "%{[@metadata][docId]}"
        index => 'hn-stories-comments'
    }
}
